{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um-RcSpkey91"
      },
      "source": [
        "##  Python for Data Engineers in 1 HOUR! Full Course + Programming Tutorial - https://youtu.be/IJm--UbuSaM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky8pUEMQ1pSC"
      },
      "source": [
        "# Section 2: Setting Up the Python Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "fLyJFN4U1wIO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lhhl8nCE5-Wp"
      },
      "outputs": [],
      "source": [
        "# Create a directory for all datasets\n",
        "os.makedirs(\"datasets\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wrBAxdoZ3LFp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets created in the 'datasets' directory:\n",
            "- weather_data.csv\n",
            "- sales_data.xlsx\n",
            "- titanic.csv\n",
            "- product_data.csv\n",
            "- employees.json\n",
            "- user_purchases.parquet\n"
          ]
        }
      ],
      "source": [
        "# 1. Titanic Sample Data (CSV)\n",
        "titanic_data = {\n",
        "    \"PassengerId\": [1, 2, 3, 4, 5, 3, 5],  # Duplicate Passenger IDs for demonstration\n",
        "    \"Survived\": [0, 1, 1, 0, 1, 1, 1],\n",
        "    \"Pclass\": [3, 1, 3, 1, 3, 3, 3],\n",
        "    \"Name\": [\"John Doe\", \"Jane Smith\", \"Alice Brown\", \"William Johnson\", \"Linda Lee\", \"Alice Brown\", \"Linda Lee\"],  # Duplicate Names\n",
        "    \"Sex\": [\"male\", \"female\", \"female\", \"male\", \"female\", \"female\", \"female\"],\n",
        "    \"Age\": [22, 38, None, 35, None, None, 28],  # Adding null values in Age\n",
        "    \"Fare\": [7.25, 71.2833, None, 53.1, 8.05, None, 8.05]  # Adding null values in Fare, with duplicates\n",
        "}\n",
        "\n",
        "df_titanic = pd.DataFrame(titanic_data)\n",
        "df_titanic.to_csv(\"datasets/titanic.csv\", index=False)\n",
        "\n",
        "# 2. Employee Data (JSON)\n",
        "json_data = [\n",
        "    {\n",
        "        \"employee_id\": 101,\n",
        "        \"name\": \"Alice\",\n",
        "        \"department\": \"Engineering\",\n",
        "        \"salary\": 70000,\n",
        "        \"projects\": [\n",
        "            {\"name\": \"Project X\", \"status\": \"Completed\"},\n",
        "            {\"name\": \"Project Y\", \"status\": \"In Progress\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"employee_id\": 102,\n",
        "        \"name\": \"Bob\",\n",
        "        \"department\": \"Sales\",\n",
        "        \"salary\": 55000,\n",
        "        \"projects\": [\n",
        "            {\"name\": \"Project A\", \"status\": \"Completed\"},\n",
        "            {\"name\": \"Project B\", \"status\": \"Completed\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "with open(\"datasets/employees.json\", \"w\") as json_file:\n",
        "    json.dump(json_data, json_file, indent=4)\n",
        "\n",
        "# 3. Sales Data (Excel)\n",
        "sales_data = {\n",
        "    \"Date\": pd.date_range(start=\"2024-01-01\", periods=10, freq=\"D\"),\n",
        "    \"Sales\": np.random.randint(100, 1000, size=10),\n",
        "    \"Product\": [\"Widget A\", \"Widget B\"] * 5\n",
        "}\n",
        "df_sales = pd.DataFrame(sales_data)\n",
        "df_sales.to_excel(\"datasets/sales_data.xlsx\", index=False)\n",
        "\n",
        "# 4. Large User Purchase Data (Parquet)\n",
        "large_data = {\n",
        "    \"user_id\": np.arange(1, 1001),\n",
        "    \"age\": np.random.randint(18, 65, 1000),\n",
        "    \"location\": np.random.choice([\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"], 1000),\n",
        "    \"purchase_amount\": np.random.uniform(5.0, 500.0, 1000)\n",
        "}\n",
        "df_large = pd.DataFrame(large_data)\n",
        "df_large.to_parquet(\"datasets/user_purchases.parquet\", index=False)\n",
        "\n",
        "# 5. Product Data (CSV)\n",
        "product_data = {\n",
        "    \"Product\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
        "    \"Price\": [9.99, 14.99, 7.99, 24.99, 4.99],\n",
        "    \"Stock\": [100, 50, 150, 30, 200]\n",
        "}\n",
        "df_product = pd.DataFrame(product_data)\n",
        "df_product.to_csv(\"datasets/product_data.csv\", index=False)\n",
        "\n",
        "# 6. Weather Data (Time-Series CSV)\n",
        "date_time_data = {\n",
        "    \"timestamp\": pd.date_range(start=\"2024-01-01\", periods=12, freq=\"ME\"),\n",
        "    \"temperature\": np.random.uniform(-10, 30, 12),\n",
        "    \"humidity\": np.random.uniform(30, 90, 12)\n",
        "}\n",
        "df_weather = pd.DataFrame(date_time_data)\n",
        "df_weather.to_csv(\"datasets/weather_data.csv\", index=False)\n",
        "\n",
        "# List the created datasets\n",
        "print(\"Datasets created in the 'datasets' directory:\")\n",
        "for file in os.listdir(\"datasets\"):\n",
        "    print(f\"- {file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3rtEEil2Sg3"
      },
      "source": [
        "# Section 3: Python Basics for Data Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yWB64zkx2SC0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable examples: 10 10.5 Alice False\n"
          ]
        }
      ],
      "source": [
        "# 3.1 Variables and Data Types\n",
        "\n",
        "# Integer\n",
        "age = 10\n",
        "\n",
        "# Float\n",
        "height = 10.5\n",
        "\n",
        "# String\n",
        "name = \"Alice\"\n",
        "\n",
        "# Boolean\n",
        "is_student = False\n",
        "\n",
        "print(\"Variable examples:\", age, height, name, is_student)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lfdGsf0n10R7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Addition: 13\n",
            "Division: 3.3333333333333335\n",
            "Modulus: 1\n",
            "Exponent: 1000\n"
          ]
        }
      ],
      "source": [
        "# 3.2 Operators\n",
        "\n",
        "# Arithmetic Operators\n",
        "a = 10\n",
        "b = 3\n",
        "print(\"Addition:\", a + b)\n",
        "print(\"Division:\", a / b)\n",
        "print(\"Modulus:\", a % b)\n",
        "print(\"Exponent:\", a ** b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wt5QIR7sfM3C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n",
            "Hello World\n",
            "HelloHelloHello\n"
          ]
        }
      ],
      "source": [
        "# Operator behavior with different data types\n",
        "\n",
        "# Addition with numbers\n",
        "num1 = 10\n",
        "num2 = 20\n",
        "print(num1 + num2)\n",
        "\n",
        "# Concatenation with strings\n",
        "str1 = \"Hello\"\n",
        "str2 = \"World\"\n",
        "print(str1 + \" \" + str2)\n",
        "\n",
        "# Repetition with strings\n",
        "print(str1 * 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xWS5DCuhfKE2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Equal: False\n",
            "Greater than: True\n"
          ]
        }
      ],
      "source": [
        "# Comparison Operators\n",
        "print(\"Equal:\", a == b)\n",
        "print(\"Greater than:\", a > b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BXCFoKJcfLef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logical AND: False\n",
            "Logical OR: True\n"
          ]
        }
      ],
      "source": [
        "# Logical Operators\n",
        "x = True\n",
        "y = False\n",
        "print(\"Logical AND:\", x and y)\n",
        "print(\"Logical OR:\", x or y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K6PILgE52g8u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Product is in high stock.\n"
          ]
        }
      ],
      "source": [
        "# 3.3 Control Strcutures\n",
        "\n",
        "# Check if a product is in stock from our product_data dataset\n",
        "product_stock = 150  # Example stock quantity\n",
        "if product_stock > 100:\n",
        "    print(\"Product is in high stock.\")\n",
        "elif product_stock > 50:\n",
        "    print(\"Product is in moderate stock.\")\n",
        "else:\n",
        "    print(\"Product is in low stock.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Er-yPqw32gHU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Product A has 100 items in stock.\n",
            "Product B has 50 items in stock.\n",
            "Product C has 150 items in stock.\n",
            "Product D has 30 items in stock.\n",
            "Product E has 200 items in stock.\n"
          ]
        }
      ],
      "source": [
        "# Iterate through a list of products and check stock availability (example from product_data.csv)\n",
        "products = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
        "stock_levels = [100, 50, 150, 30, 200]\n",
        "\n",
        "for product, stock in zip(products, stock_levels):\n",
        "    print(f\"Product {product} has {stock} items in stock.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "O1kFlbNK5A_1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count: 0\n",
            "Count: 1\n",
            "Count: 2\n",
            "Count: 3\n",
            "Count: 4\n"
          ]
        }
      ],
      "source": [
        "# While loop example\n",
        "count = 0\n",
        "while count < 5:\n",
        "    print(\"Count:\", count)\n",
        "    count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mNu8i_1w2pAi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average stock level: 106.0\n"
          ]
        }
      ],
      "source": [
        "# 3.4 Functions, Modules, and Packages\n",
        "\n",
        "# Function to calculate average stock from product data\n",
        "def calculate_average_stock(stock_list):\n",
        "    total_stock = sum(stock_list)\n",
        "    return total_stock / len(stock_list)\n",
        "\n",
        "# Using the function with stock levels from product_data\n",
        "average_stock = calculate_average_stock(stock_levels)\n",
        "print(\"Average stock level:\", average_stock)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qAu-YSr622P1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Square root of average stock: 10.295630140987\n"
          ]
        }
      ],
      "source": [
        "# Using Modules and Packages\n",
        "\n",
        "import math\n",
        "\n",
        "# Calculate the square root of an average stock level\n",
        "print(\"Square root of average stock:\", math.sqrt(average_stock))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mmd1c69qfrPX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "# Lambda Functions\n",
        "\n",
        "# Syntax example for a lambda function\n",
        "add = lambda x, y: x + y  # Adds two numbers\n",
        "print(add(3, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rVF1PIGR3dDZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 4, 9, 16]\n"
          ]
        }
      ],
      "source": [
        "# Using a lambda function with the map() function\n",
        "numbers = [1, 2, 3, 4]\n",
        "squared = list(map(lambda x: x**2, numbers))  # Squares each number in the list\n",
        "print(squared)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8K_u_z683eT1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['HELLO', 'WORLD']\n"
          ]
        }
      ],
      "source": [
        "# Another example: converting a list of strings to uppercase\n",
        "words = [\"hello\", \"world\"]\n",
        "uppercase_words = list(map(lambda x: x.upper(), words))  # Converts each word to uppercase\n",
        "print(uppercase_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "u820Zs0f2623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, Alice Johnson!\n"
          ]
        }
      ],
      "source": [
        "# 3.5 String Manipulation Techniques\n",
        "\n",
        "# Example employee data from employees.json\n",
        "employee_name = \"Alice Johnson\"\n",
        "\n",
        "# Concatenation\n",
        "greeting = \"Hello, \" + employee_name + \"!\"\n",
        "print(greeting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "e7j2Uu563gWk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alice Johnson works in the Engineering department.\n"
          ]
        }
      ],
      "source": [
        "# String formatting with f-strings\n",
        "department = \"Engineering\"\n",
        "print(f\"{employee_name} works in the {department} department.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xlVb2EYC3ha1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 3 characters: Dat\n"
          ]
        }
      ],
      "source": [
        "# Slicing\n",
        "sample_text = \"Data Engineering\"\n",
        "print(\"First 3 characters:\", sample_text[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YTl6yp7n3iZz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lowercase: data engineering\n",
            "Uppercase: DATA ENGINEERING\n",
            "Replace: Information Engineering\n"
          ]
        }
      ],
      "source": [
        "# Methods\n",
        "print(\"Lowercase:\", sample_text.lower())\n",
        "print(\"Uppercase:\", sample_text.upper())\n",
        "print(\"Replace:\", sample_text.replace(\"Data\", \"Information\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "clQmXVNt2-wC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File not found. Please check the file path.\n",
            "Execution completed.\n"
          ]
        }
      ],
      "source": [
        "# 3.6 Error Handling\n",
        "\n",
        "# Error handling when reading a non-existent file\n",
        "try:\n",
        "    with open(\"datasets/nonexistent_file.csv\", \"r\") as file:\n",
        "        content = file.read()\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found. Please check the file path.\")\n",
        "else:\n",
        "    print(\"File read successfully.\")\n",
        "finally:\n",
        "    print(\"Execution completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaUzTV2c4QRj"
      },
      "source": [
        "# Section 4: Python Collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Pv-uLOr24Ul9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latitude: 51.5074, Longitude: -0.1278\n"
          ]
        }
      ],
      "source": [
        "# 4.2 Tuples\n",
        "\n",
        "# Defining a tuple\n",
        "coordinates = (51.5074, -0.1278)  # Coordinates for London\n",
        "\n",
        "# Accessing tuple elements\n",
        "latitude = coordinates[0]\n",
        "longitude = coordinates[1]\n",
        "\n",
        "print(f\"Latitude: {latitude}, Longitude: {longitude}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YTK465Zl4abJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated product names: ['Widget A', 'Widget C', 'Widget D']\n"
          ]
        }
      ],
      "source": [
        "# 4.3 Lists\n",
        "\n",
        "# Defining a list of product names\n",
        "product_names = [\"Widget A\", \"Widget B\", \"Widget C\"]\n",
        "\n",
        "# Adding a new product\n",
        "product_names.append(\"Widget D\")\n",
        "\n",
        "# Removing a product\n",
        "product_names.remove(\"Widget B\")\n",
        "\n",
        "# Printing the updated list\n",
        "print(\"Updated product names:\", product_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "S1xJmNjp5jJ_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique product IDs: {101, 103, 104, 105}\n"
          ]
        }
      ],
      "source": [
        "# 4.4 Sets\n",
        "\n",
        "# Defining a set of unique product IDs\n",
        "product_ids = {101, 102, 103, 104, 101}  # Duplicates are ignored\n",
        "\n",
        "# Adding a new unique ID\n",
        "product_ids.add(105)\n",
        "\n",
        "# Removing an ID\n",
        "product_ids.discard(102)\n",
        "\n",
        "print(\"Unique product IDs:\", product_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "kl-xN47f5lsK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Product Information: {'Product ID': 101, 'Name': 'Widget A', 'Price': 9.99, 'Stock': 120, 'Category': 'Gadgets'}\n"
          ]
        }
      ],
      "source": [
        "# 4.5 Dictionaries\n",
        "\n",
        "# Defining a dictionary for a product with key-value pairs\n",
        "product_info = {\n",
        "    \"Product ID\": 101,\n",
        "    \"Name\": \"Widget A\",\n",
        "    \"Price\": 9.99,\n",
        "    \"Stock\": 100\n",
        "}\n",
        "\n",
        "# Accessing values by keys\n",
        "product_name = product_info[\"Name\"]\n",
        "product_price = product_info[\"Price\"]\n",
        "\n",
        "# Adding a new key-value pair\n",
        "product_info[\"Category\"] = \"Gadgets\"\n",
        "\n",
        "# Updating the stock count\n",
        "product_info[\"Stock\"] = 120\n",
        "\n",
        "print(\"Product Information:\", product_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_vf9zPGA5p_z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Sales by Product: {'Widget A': 500, 'Widget B': 150}\n"
          ]
        }
      ],
      "source": [
        "# 4.6 Handling Data Strcutures Effectively\n",
        "\n",
        "# Sample sales data representing rows from sales_data.xlsx\n",
        "sales_records = [\n",
        "    {\"Date\": \"2024-01-01\", \"Product\": \"Widget A\", \"Sales\": 200},\n",
        "    {\"Date\": \"2024-01-02\", \"Product\": \"Widget B\", \"Sales\": 150},\n",
        "    {\"Date\": \"2024-01-03\", \"Product\": \"Widget A\", \"Sales\": 300},\n",
        "]\n",
        "\n",
        "# Calculate total sales for each product\n",
        "total_sales = {}\n",
        "for record in sales_records:\n",
        "    product = record[\"Product\"]\n",
        "    sales = record[\"Sales\"]\n",
        "    if product in total_sales:\n",
        "        total_sales[product] += sales\n",
        "    else:\n",
        "        total_sales[product] = sales\n",
        "\n",
        "print(\"Total Sales by Product:\", total_sales)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAWQfWaq6qu1"
      },
      "source": [
        "# 5. File Handling in Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "z7xgs2ia5y7D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content of the text file:\n",
            " Hello, Data Engineering!\n",
            "Working with text files in Python.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5.1 Text Files (.txt)\n",
        "\n",
        "# Writing to a text file\n",
        "with open(\"datasets/sample_text.txt\", \"w\") as file:\n",
        "    file.write(\"Hello, Data Engineering!\\n\")\n",
        "    file.write(\"Working with text files in Python.\\n\")\n",
        "\n",
        "# Reading from a text file\n",
        "with open(\"datasets/sample_text.txt\", \"r\") as file:\n",
        "    content = file.read()\n",
        "    print(\"Content of the text file:\\n\", content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "VADzHsGt6xtO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content of product_data.csv:\n",
            "     Product  Price  Stock\n",
            "0  Widget A  10.99     50\n",
            "1  Widget B  14.99     20\n",
            "2  Widget C   7.99    100\n"
          ]
        }
      ],
      "source": [
        "# 5.2 CSV Files (.csv - Comma Separated Values)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Writing a DataFrame to a CSV file\n",
        "product_data = pd.DataFrame({\n",
        "    \"Product\": [\"Widget A\", \"Widget B\", \"Widget C\"],\n",
        "    \"Price\": [10.99, 14.99, 7.99],\n",
        "    \"Stock\": [50, 20, 100]\n",
        "})\n",
        "product_data.to_csv(\"datasets/product_data.csv\", index=False)\n",
        "\n",
        "# Reading from a CSV file\n",
        "df = pd.read_csv(\"datasets/product_data.csv\")\n",
        "print(\"Content of product_data.csv:\\n\", df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mRWrrX3-63gR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content of employees.json:\n",
            " [{'employee_id': 101, 'name': 'Alice', 'department': 'Engineering', 'projects': ['Project X', 'Project Y']}, {'employee_id': 102, 'name': 'Bob', 'department': 'Sales', 'projects': ['Project A', 'Project B']}]\n"
          ]
        }
      ],
      "source": [
        "# 5.3 JSON Files (.json - JavaScript Object Notation)\n",
        "\n",
        "import json\n",
        "\n",
        "# Writing to a JSON file\n",
        "employee_data = [\n",
        "    {\n",
        "        \"employee_id\": 101,\n",
        "        \"name\": \"Alice\",\n",
        "        \"department\": \"Engineering\",\n",
        "        \"projects\": [\"Project X\", \"Project Y\"]\n",
        "    },\n",
        "    {\n",
        "        \"employee_id\": 102,\n",
        "        \"name\": \"Bob\",\n",
        "        \"department\": \"Sales\",\n",
        "        \"projects\": [\"Project A\", \"Project B\"]\n",
        "    }\n",
        "]\n",
        "with open(\"datasets/employees.json\", \"w\") as json_file:\n",
        "    json.dump(employee_data, json_file, indent=4)\n",
        "\n",
        "# Reading from a JSON file\n",
        "with open(\"datasets/employees.json\", \"r\") as json_file:\n",
        "    data = json.load(json_file)\n",
        "    print(\"Content of employees.json:\\n\", data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "O7U0kFm669aW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content of sales_data.xlsx:\n",
            "         Date   Product  Sales\n",
            "0 2024-01-01  Widget A    200\n",
            "1 2024-01-02  Widget B    150\n",
            "2 2024-01-03  Widget A    300\n",
            "3 2024-01-04  Widget B    250\n",
            "4 2024-01-05  Widget C    100\n"
          ]
        }
      ],
      "source": [
        "# 5.4 Excel Files (.xlsx)\n",
        "\n",
        "# Writing to an Excel file\n",
        "sales_data = pd.DataFrame({\n",
        "    \"Date\": pd.date_range(start=\"2024-01-01\", periods=5, freq=\"D\"),\n",
        "    \"Product\": [\"Widget A\", \"Widget B\", \"Widget A\", \"Widget B\", \"Widget C\"],\n",
        "    \"Sales\": [200, 150, 300, 250, 100]\n",
        "})\n",
        "sales_data.to_excel(\"datasets/sales_data.xlsx\", index=False)\n",
        "\n",
        "# Reading from an Excel file\n",
        "df_sales = pd.read_excel(\"datasets/sales_data.xlsx\")\n",
        "print(\"Content of sales_data.xlsx:\\n\", df_sales)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "eGlp-yJ57CYL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content of user_purchases.parquet:\n",
            "    user_id  age  purchase_amount\n",
            "0        1   23           100.50\n",
            "1        2   35           200.75\n",
            "2        3   45           300.10\n",
            "3        4   30           150.00\n",
            "4        5   27           250.50\n"
          ]
        }
      ],
      "source": [
        "# 5.5 Parquet Files (.parquet)\n",
        "\n",
        "# Writing to a Parquet file\n",
        "user_data = pd.DataFrame({\n",
        "    \"user_id\": range(1, 6),\n",
        "    \"age\": [23, 35, 45, 30, 27],\n",
        "    \"purchase_amount\": [100.5, 200.75, 300.1, 150.0, 250.5]\n",
        "})\n",
        "user_data.to_parquet(\"datasets/user_purchases.parquet\", index=False)\n",
        "\n",
        "# Reading from a Parquet file\n",
        "df_user = pd.read_parquet(\"datasets/user_purchases.parquet\")\n",
        "print(\"Content of user_purchases.parquet:\\n\", df_user)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNjuRorc7Miy"
      },
      "source": [
        "# Section 6 - Data Processing with Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vWZI1z6R7ESg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few rows of Titanic dataset:\n",
            "    PassengerId  Survived  Pclass             Name     Sex   Age     Fare\n",
            "0            1         0       3         John Doe    male  22.0   7.2500\n",
            "1            2         1       1       Jane Smith  female  38.0  71.2833\n",
            "2            3         1       3      Alice Brown  female   NaN      NaN\n",
            "3            4         0       1  William Johnson    male  35.0  53.1000\n",
            "4            5         1       3        Linda Lee  female   NaN   8.0500\n"
          ]
        }
      ],
      "source": [
        "# 6.1 Introduction to Pandas and DataFrames\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Loading the Titanic dataset\n",
        "df_titanic = pd.read_csv(\"datasets/titanic.csv\")\n",
        "print(\"First few rows of Titanic dataset:\\n\", df_titanic.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "71R5MW5xDtcK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values:\n",
            " PassengerId    0\n",
            "Survived       0\n",
            "Pclass         0\n",
            "Name           0\n",
            "Sex            0\n",
            "Age            3\n",
            "Fare           2\n",
            "dtype: int64\n",
            "Data after handling missing values:\n",
            " PassengerId    0\n",
            "Survived       0\n",
            "Pclass         0\n",
            "Name           0\n",
            "Sex            0\n",
            "Age            1\n",
            "Fare           0\n",
            "dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/j6/k82_mc292vq_vpp9f821cv980000gn/T/ipykernel_3969/2308338079.py:7: ChainedAssignmentError: A value is being set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "Such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy (due to Copy-on-Write).\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object, or try to avoid an inplace operation using 'df[col] = df[col].method(value)'.\n",
            "\n",
            "See the documentation for a more detailed explanation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html\n",
            "  df_titanic['Age'].fillna(df_titanic['Age'].median(), inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# 6.2 Data Cleaning and Preprocessing Techniques\n",
        "\n",
        "# Checking for missing values in Titanic dataset\n",
        "print(\"Missing values:\\n\", df_titanic.isnull().sum())\n",
        "\n",
        "# Filling missing values in the 'Age' column with the column's median\n",
        "df_titanic['Age'].fillna(df_titanic['Age'].median(), inplace=True)\n",
        "\n",
        "# Dropping rows with any missing values in critical columns\n",
        "df_titanic.dropna(subset=['Fare'], inplace=True)\n",
        "\n",
        "print(\"Data after handling missing values:\\n\", df_titanic.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "W8iPit-QD96J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data after removing duplicates:\n",
            " (5, 7)\n"
          ]
        }
      ],
      "source": [
        "# Removing duplicate rows\n",
        "\n",
        "df_titanic.drop_duplicates(inplace=True)\n",
        "print(\"Data after removing duplicates:\\n\", df_titanic.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "WqJjckJ1ETr0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "High fare passengers:\n",
            "    PassengerId  Survived  Pclass             Name     Sex   Age     Fare\n",
            "1            2         1       1       Jane Smith  female  38.0  71.2833\n",
            "3            4         0       1  William Johnson    male  35.0  53.1000\n"
          ]
        }
      ],
      "source": [
        "# 6.3 Data Manipulation and Aggregation\n",
        "\n",
        "# Filtering passengers with fare above 50\n",
        "high_fare_passengers = df_titanic[df_titanic['Fare'] > 50]\n",
        "print(\"High fare passengers:\\n\", high_fare_passengers.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qfp2UgUQ6SD_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data sorted by age:\n",
            "    PassengerId  Survived  Pclass             Name     Sex   Age     Fare\n",
            "1            2         1       1       Jane Smith  female  38.0  71.2833\n",
            "3            4         0       1  William Johnson    male  35.0  53.1000\n",
            "6            5         1       3        Linda Lee  female  28.0   8.0500\n",
            "0            1         0       3         John Doe    male  22.0   7.2500\n",
            "4            5         1       3        Linda Lee  female   NaN   8.0500\n"
          ]
        }
      ],
      "source": [
        "# Sorting data by 'Age' in descending order\n",
        "df_titanic_sorted = df_titanic.sort_values(by='Age', ascending=False)\n",
        "print(\"Data sorted by age:\\n\", df_titanic_sorted.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3kpxyi4k6TP4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Fare and Age by Passenger Class:\n",
            "    Pclass       Fare   Age\n",
            "0       1  62.191650  36.5\n",
            "1       3   7.783333  25.0\n"
          ]
        }
      ],
      "source": [
        "# Grouping by 'Pclass' and calculating average 'Fare' and 'Age'\n",
        "grouped_data = df_titanic.groupby('Pclass')[['Fare', 'Age']].mean().reset_index()\n",
        "print(\"Average Fare and Age by Passenger Class:\\n\", grouped_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "-auqCcgGEdBY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matplotlib is building the font cache; this may take a moment.\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 6.4 Basic Data Visualization for Quick Insights\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting a histogram of the 'Age' column\n",
        "df_titanic['Age'].plot(kind='hist', bins=10, title='Age Distribution')\n",
        "plt.xlabel('Age')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZARLC3qmEfKp"
      },
      "outputs": [],
      "source": [
        "# Bar plot of average fare by passenger class\n",
        "grouped_data.plot(kind='bar', x='Pclass', y='Fare', title='Average Fare by Passenger Class')\n",
        "plt.xlabel('Passenger Class')\n",
        "plt.ylabel('Average Fare')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iE_ZCb_F5f4"
      },
      "source": [
        "# Section 7: Numerical Computing with NumPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "00Iw--63EhSR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1D Array: [1 2 3 4 5]\n",
            "2D Array:\n",
            " [[1 2 3]\n",
            " [4 5 6]\n",
            " [7 8 9]]\n"
          ]
        }
      ],
      "source": [
        "# 7.1 Basics of NumPy Arrays\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 1D array from a list\n",
        "array_1d = np.array([1, 2, 3, 4, 5])\n",
        "print(\"1D Array:\", array_1d)\n",
        "\n",
        "# 2D array from a list of lists\n",
        "array_2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print(\"2D Array:\\n\", array_2d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "tCox69ZbGAWu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Addition: [11 22 33]\n",
            "Multiplication: [10 40 90]\n",
            "Square root of 'a': [3.16227766 4.47213595 5.47722558]\n"
          ]
        }
      ],
      "source": [
        "# 7.2 Array Operations\n",
        "\n",
        "a = np.array([10, 20, 30])\n",
        "b = np.array([1, 2, 3])\n",
        "\n",
        "# Element-wise addition\n",
        "print(\"Addition:\", a + b)\n",
        "\n",
        "# Element-wise multiplication\n",
        "print(\"Multiplication:\", a * b)\n",
        "\n",
        "# Element-wise square root\n",
        "print(\"Square root of 'a':\", np.sqrt(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ZPWL4-p6GG81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First element: 10\n",
            "Last element: 50\n",
            "First three elements: [10 20 30]\n",
            "Elements greater than 25: [30 40 50]\n"
          ]
        }
      ],
      "source": [
        "# 7.3 Indexing and Slicing\n",
        "\n",
        "# 1D array\n",
        "array_1d = np.array([10, 20, 30, 40, 50])\n",
        "\n",
        "# Accessing the first and last element\n",
        "print(\"First element:\", array_1d[0])\n",
        "print(\"Last element:\", array_1d[-1])\n",
        "\n",
        "# Slicing to access the first three elements\n",
        "print(\"First three elements:\", array_1d[:3])\n",
        "\n",
        "# Boolean indexing to find elements greater than 25\n",
        "print(\"Elements greater than 25:\", array_1d[array_1d > 25])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "fZzRFcHwfg5x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "50\n",
            "[10, 20, 30]\n",
            "[30, 40, 50]\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "# Indexing and slicing examples\n",
        "arr = [10, 20, 30, 40, 50]\n",
        "\n",
        "# Accessing elements\n",
        "print(arr[0])\n",
        "print(arr[-1])\n",
        "\n",
        "# Slicing\n",
        "print(arr[0:3])\n",
        "print(arr[2:])\n",
        "\n",
        "# Pandas indexing\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n",
        "print(df.iloc[0, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "8Lem_hjjGOrN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matrix Product:\n",
            " [[19 22]\n",
            " [43 50]]\n"
          ]
        }
      ],
      "source": [
        "# 7.4 Linear Algebra Operations\n",
        "\n",
        "# Define two matrices\n",
        "matrix_a = np.array([[1, 2], [3, 4]])\n",
        "matrix_b = np.array([[5, 6], [7, 8]])\n",
        "\n",
        "# Matrix multiplication\n",
        "print(\"Matrix Product:\\n\", np.dot(matrix_a, matrix_b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "tV1N5MibGU_y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean: 40.0\n",
            "Median: 40.0\n",
            "Standard Deviation: 17.07825127659933\n",
            "Variance: 291.6666666666667\n",
            "Cumulative sum: [ 15  40  75 120 175 240]\n"
          ]
        }
      ],
      "source": [
        "# 7.5 Statistical Functions\n",
        "\n",
        "# Generating an array of sample data\n",
        "sample_data = np.array([15, 25, 35, 45, 55, 65])\n",
        "\n",
        "# Calculating mean, median, and standard deviation\n",
        "print(\"Mean:\", np.mean(sample_data))\n",
        "print(\"Median:\", np.median(sample_data))\n",
        "print(\"Standard Deviation:\", np.std(sample_data))\n",
        "print(\"Variance:\", np.var(sample_data))\n",
        "print(\"Cumulative sum:\", np.cumsum(sample_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OAiheefG6S_"
      },
      "source": [
        "# Section 8: Working with Dates and Times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "aRzUTIOyGZOi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsed datetime object: 2024-11-12 17:54:27\n"
          ]
        }
      ],
      "source": [
        "# 8.1 Parsing and Formatting Date Time Data\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Parsing a date-time string to a datetime object\n",
        "date_str = \"2024-11-12 17:54:27\"\n",
        "date_obj = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
        "print(\"Parsed datetime object:\", date_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "GUpXHOj6Aa3X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatted date string: November 12, 2024 - 05:54 PM\n"
          ]
        }
      ],
      "source": [
        "# Formatting a datetime object to a string\n",
        "formatted_date = date_obj.strftime(\"%B %d, %Y - %I:%M %p\")\n",
        "print(\"Formatted date string:\", formatted_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "y3Z0lK4OHASl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current date: 2026-02-03 14:26:14.803294\n",
            "Future date: 2026-02-08 14:26:14.803294\n"
          ]
        }
      ],
      "source": [
        "# 8.2 Common Date Time Operations\n",
        "\n",
        "from datetime import timedelta\n",
        "\n",
        "# Adding 5 days to the current date\n",
        "current_date = datetime.now()\n",
        "future_date = current_date + timedelta(days=5)\n",
        "print(\"Current date:\", current_date)\n",
        "print(\"Future date:\", future_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "WmOZ6VfbAc0v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Year: 2024\n",
            "Month: 11\n",
            "Day: 12\n",
            "Hour: 17\n",
            "Minute: 54\n",
            "Second: 27\n"
          ]
        }
      ],
      "source": [
        "# Extracting specific components\n",
        "print(\"Year:\", date_obj.year)\n",
        "print(\"Month:\", date_obj.month)\n",
        "print(\"Day:\", date_obj.day)\n",
        "print(\"Hour:\", date_obj.hour)\n",
        "print(\"Minute:\", date_obj.minute)\n",
        "print(\"Second:\", date_obj.second)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "TapngozYHGn3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame with parsed timestamps:\n",
            "    timestamp  temperature   humidity\n",
            "0 2024-01-31    -7.132096  66.342265\n",
            "1 2024-02-29    20.373321  63.676032\n",
            "2 2024-03-31     9.241804  59.675997\n",
            "3 2024-04-30    26.324211  60.733805\n",
            "4 2024-05-31     2.542301  88.510310\n"
          ]
        }
      ],
      "source": [
        "# 8.3 Handling Date Time Data in ETL Pipelines\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the weather data\n",
        "df = pd.read_csv(\"datasets/weather_data.csv\")\n",
        "\n",
        "# Parsing date-time columns if necessary\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "print(\"DataFrame with parsed timestamps:\\n\", df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "4YOtZpBa6qKX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered DataFrame:\n",
            "    timestamp  temperature   humidity\n",
            "0 2024-01-31    -7.132096  66.342265\n",
            "1 2024-02-29    20.373321  63.676032\n",
            "2 2024-03-31     9.241804  59.675997\n",
            "3 2024-04-30    26.324211  60.733805\n",
            "4 2024-05-31     2.542301  88.510310\n",
            "5 2024-06-30    28.669451  39.885403\n"
          ]
        }
      ],
      "source": [
        "# Filtering data within a specific date range\n",
        "start_date = '2024-01-01'\n",
        "end_date = '2024-06-30'\n",
        "filtered_df = df[(df['timestamp'] >= start_date) & (df['timestamp'] <= end_date)]\n",
        "print(\"Filtered DataFrame:\\n\", filtered_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "P0Oxq27r6rfH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame with time differences:\n",
            "     timestamp  temperature   humidity time_diff\n",
            "0  2024-01-31    -7.132096  66.342265       NaT\n",
            "1  2024-02-29    20.373321  63.676032   29 days\n",
            "2  2024-03-31     9.241804  59.675997   31 days\n",
            "3  2024-04-30    26.324211  60.733805   30 days\n",
            "4  2024-05-31     2.542301  88.510310   31 days\n",
            "5  2024-06-30    28.669451  39.885403   30 days\n",
            "6  2024-07-31    -6.321842  66.795631   31 days\n",
            "7  2024-08-31    14.929582  42.067419   31 days\n",
            "8  2024-09-30    18.029127  56.544924   30 days\n",
            "9  2024-10-31    11.710964  47.390150   31 days\n",
            "10 2024-11-30    28.959626  65.474708   30 days\n",
            "11 2024-12-31     1.549715  56.012882   31 days\n"
          ]
        }
      ],
      "source": [
        "# Calculating time differences between rows\n",
        "df['time_diff'] = df['timestamp'].diff()\n",
        "print(\"DataFrame with time differences:\\n\", df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbABvoFdHkwf"
      },
      "source": [
        "# Section 9 - Working with APIs and External Connections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1akbr3zh2gVZ"
      },
      "source": [
        "**Note:** for this section to work you will have to create a .env file with WEATHER_API_KEY = \"your_key\". You can get your free key from https://weatherapi.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "n3NSNe0hLhgq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (1.2.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "nkCCSUZiHNkM"
      },
      "outputs": [],
      "source": [
        "# 9.2 Setting Up API Requests in Python\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Retrieve the API key from environment variables\n",
        "api_key = os.getenv(\"WEATHER_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "PEtQA85vHteu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to retrieve data. Status code: 401\n"
          ]
        }
      ],
      "source": [
        "# 9.3 Making API Requests: GET Requests\n",
        "\n",
        "# Define the API endpoint and parameters\n",
        "api_url = \"http://api.weatherapi.com/v1/current.json\"\n",
        "params = {\n",
        "    \"key\": api_key,\n",
        "    \"q\": \"London\",               # City to get the weather for\n",
        "    \"aqi\": \"no\"                  # Air Quality Index (optional)\n",
        "}\n",
        "\n",
        "# Making the GET request\n",
        "response = requests.get(api_url, params=params)\n",
        "\n",
        "# Checking the response status\n",
        "if response.status_code == 200:\n",
        "    print(\"Data retrieved successfully!\")\n",
        "    data = response.json()  # Convert JSON response to Python dictionary\n",
        "    print(data)\n",
        "else:\n",
        "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "K5ZMl9TbJBRC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: 401 Client Error: Unauthorized for url: http://api.weatherapi.com/v1/current.json?q=London&aqi=no\n"
          ]
        }
      ],
      "source": [
        "# 9.4 Handling API Responses and Errors\n",
        "\n",
        "try:\n",
        "    # Making the GET request with a timeout\n",
        "    response = requests.get(api_url, params=params, timeout=5)\n",
        "    response.raise_for_status()  # Raises an error for unsuccessful status codes\n",
        "    data = response.json()\n",
        "    print(\"Data retrieved:\", data)\n",
        "except requests.exceptions.Timeout:\n",
        "    print(\"Request timed out. Try again later.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(\"An error occurred:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ViYpi6l4JJVK"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not str",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Assuming we received data from the API\u001b[39;00m\n\u001b[32m      6\u001b[39m weather_data = {\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcity\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mLondon\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtemperature_celsius\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcurrent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtemp_c\u001b[39m\u001b[33m\"\u001b[39m]],\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhumidity\u001b[39m\u001b[33m\"\u001b[39m: [data[\u001b[33m\"\u001b[39m\u001b[33mcurrent\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mhumidity\u001b[39m\u001b[33m\"\u001b[39m]],\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcondition\u001b[39m\u001b[33m\"\u001b[39m: [data[\u001b[33m\"\u001b[39m\u001b[33mcurrent\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcondition\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m     11\u001b[39m }\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[32m     14\u001b[39m df_weather = pd.DataFrame(weather_data)\n",
            "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
          ]
        }
      ],
      "source": [
        "# 9.5 Saving API Data for Further Processing\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming we received data from the API\n",
        "weather_data = {\n",
        "    \"city\": [\"London\"],\n",
        "    \"temperature_celsius\": [data[\"current\"][\"temp_c\"]],\n",
        "    \"humidity\": [data[\"current\"][\"humidity\"]],\n",
        "    \"condition\": [data[\"current\"][\"condition\"][\"text\"]]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df_weather = pd.DataFrame(weather_data)\n",
        "\n",
        "# Save to CSV\n",
        "df_weather.to_csv(\"datasets/weather_data.csv\", index=False)\n",
        "print(\"Data saved to weather_data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "E0GTU_hiJWXm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch data for New York. Status code: 401\n",
            "No data to load.\n"
          ]
        }
      ],
      "source": [
        "# 9.6 Practical Use Case: Building an API Data Pipeline\n",
        "\n",
        "def fetch_weather_data(city, api_key):\n",
        "    \"\"\"Extract weather data for a specific city using WeatherAPI.\"\"\"\n",
        "    api_url = \"http://api.weatherapi.com/v1/current.json\"\n",
        "    params = {\"key\": api_key, \"q\": city, \"aqi\": \"no\"}\n",
        "    response = requests.get(api_url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Failed to fetch data for {city}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def transform_weather_data(data):\n",
        "    \"\"\"Transform weather data by selecting relevant fields.\"\"\"\n",
        "    if data:\n",
        "        transformed_data = {\n",
        "            \"city\": data[\"location\"][\"name\"],\n",
        "            \"temperature_celsius\": data[\"current\"][\"temp_c\"],\n",
        "            \"humidity\": data[\"current\"][\"humidity\"],\n",
        "            \"condition\": data[\"current\"][\"condition\"][\"text\"]\n",
        "        }\n",
        "        return transformed_data\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def load_data_to_csv(data, filename=\"datasets/weather_data_pipeline.csv\"):\n",
        "    \"\"\"Load transformed data into a CSV file.\"\"\"\n",
        "    if data:\n",
        "        df = pd.DataFrame([data])  # Convert to DataFrame\n",
        "        df.to_csv(filename, mode='a', header=not pd.io.common.file_exists(filename), index=False)\n",
        "        print(f\"Data loaded successfully into {filename}.\")\n",
        "    else:\n",
        "        print(\"No data to load.\")\n",
        "\n",
        "# Example usage of the ETL pipeline\n",
        "api_key = api_key\n",
        "city = \"New York\"\n",
        "\n",
        "# Extract\n",
        "raw_data = fetch_weather_data(city, api_key)\n",
        "\n",
        "# Transform\n",
        "transformed_data = transform_weather_data(raw_data)\n",
        "\n",
        "# Load\n",
        "load_data_to_csv(transformed_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsLS1lHRKfVE"
      },
      "source": [
        "# Section 10: Object-Oriented Programming (OOP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "Zz9ttQMIJhyd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Passenger John Doe, Age: 22, Class: 3, Survived: False\n",
            "Passenger Jane Smith, Age: 38, Class: 1, Survived: True\n"
          ]
        }
      ],
      "source": [
        "# 10.1 Classes and Objects in Python\n",
        "\n",
        "class Passenger:\n",
        "    def __init__(self, passenger_id, name, age, pclass, survived):\n",
        "        self.passenger_id = passenger_id\n",
        "        self.name = name\n",
        "        self.age = age\n",
        "        self.pclass = pclass\n",
        "        self.survived = survived\n",
        "\n",
        "    def display_info(self):\n",
        "        return f\"Passenger {self.name}, Age: {self.age}, Class: {self.pclass}, Survived: {self.survived}\"\n",
        "\n",
        "# Create instances of the Passenger class\n",
        "passenger1 = Passenger(1, \"John Doe\", 22, 3, False)\n",
        "passenger2 = Passenger(2, \"Jane Smith\", 38, 1, True)\n",
        "\n",
        "print(passenger1.display_info())\n",
        "print(passenger2.display_info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "3BhEmUTlKmcQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "# 10.2 Key OOP Principles\n",
        "\n",
        "# Encapsulation\n",
        "class Passenger:\n",
        "    def __init__(self, passenger_id, name, age, pclass, survived):\n",
        "        self.__passenger_id = passenger_id  # Private attribute\n",
        "        self.name = name\n",
        "        self.age = age\n",
        "        self.pclass = pclass\n",
        "        self.survived = survived\n",
        "\n",
        "    def get_passenger_id(self):\n",
        "        return self.__passenger_id\n",
        "\n",
        "# Accessing private attributes\n",
        "passenger = Passenger(1, \"John Doe\", 22, 3, False)\n",
        "print(passenger.get_passenger_id())  # Correct way to access the ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "YgKerMXfKrzu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jane Smith - Age: 38 - Class: 1\n"
          ]
        }
      ],
      "source": [
        "# Inheritance\n",
        "\n",
        "class Person:\n",
        "    def __init__(self, name, age):\n",
        "        self.name = name\n",
        "        self.age = age\n",
        "\n",
        "class Passenger(Person):  # Passenger inherits from Person\n",
        "    def __init__(self, name, age, pclass, survived):\n",
        "        super().__init__(name, age)\n",
        "        self.pclass = pclass\n",
        "        self.survived = survived\n",
        "\n",
        "passenger = Passenger(\"Jane Smith\", 38, 1, True)\n",
        "print(f\"{passenger.name} - Age: {passenger.age} - Class: {passenger.pclass}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "6urvxhEXK0sO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is a passenger.\n",
            "This is a crew member.\n"
          ]
        }
      ],
      "source": [
        "# Polymorphism\n",
        "\n",
        "class Person:\n",
        "    def info(self):\n",
        "        return \"This is a person.\"\n",
        "\n",
        "class Passenger(Person):\n",
        "    def info(self):\n",
        "        return \"This is a passenger.\"\n",
        "\n",
        "class CrewMember(Person):\n",
        "    def info(self):\n",
        "        return \"This is a crew member.\"\n",
        "\n",
        "people = [Passenger(), CrewMember()]\n",
        "for person in people:\n",
        "    print(person.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "yfllgcFFK4a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from CSV.\n"
          ]
        }
      ],
      "source": [
        "# Abstraction\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class DataLoader(ABC):  # Abstract base class\n",
        "    @abstractmethod\n",
        "    def load_data(self):\n",
        "        pass\n",
        "\n",
        "class CSVLoader(DataLoader):\n",
        "    def load_data(self):\n",
        "        return \"Loading data from CSV.\"\n",
        "\n",
        "class JSONLoader(DataLoader):\n",
        "    def load_data(self):\n",
        "        return \"Loading data from JSON.\"\n",
        "\n",
        "loader = CSVLoader()\n",
        "print(loader.load_data())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "OXUBEqBAK58S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded: {'name': 'JOHN DOE', 'age': 30}\n"
          ]
        }
      ],
      "source": [
        "# 10.3 Practical Use Cases in Data Engineering\n",
        "\n",
        "class Extract:\n",
        "    def get_data(self):\n",
        "        # Simulate data extraction\n",
        "        return {\"name\": \"John Doe\", \"age\": 30}\n",
        "\n",
        "class Transform:\n",
        "    def process_data(self, data):\n",
        "        # Simulate data transformation\n",
        "        data[\"name\"] = data[\"name\"].upper()\n",
        "        return data\n",
        "\n",
        "class Load:\n",
        "    def save_data(self, data):\n",
        "        # Simulate loading data into storage\n",
        "        print(\"Data loaded:\", data)\n",
        "\n",
        "# Using the ETL components together\n",
        "data_extractor = Extract()\n",
        "data_transformer = Transform()\n",
        "data_loader = Load()\n",
        "\n",
        "data = data_extractor.get_data()\n",
        "transformed_data = data_transformer.process_data(data)\n",
        "data_loader.save_data(transformed_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uAt_QUMQ5C"
      },
      "source": [
        "# Section 11: Building ETL Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "0F_w9-icLA1L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data extracted successfully.\n",
            "   PassengerId  Survived  Pclass             Name     Sex   Age     Fare\n",
            "0            1         0       3         John Doe    male  22.0   7.2500\n",
            "1            2         1       1       Jane Smith  female  38.0  71.2833\n",
            "2            3         1       3      Alice Brown  female   NaN      NaN\n",
            "3            4         0       1  William Johnson    male  35.0  53.1000\n",
            "4            5         1       3        Linda Lee  female   NaN   8.0500\n"
          ]
        }
      ],
      "source": [
        "# 11.2 Building a Sample ETL Pipeline in Python\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1 - Extract\n",
        "\n",
        "def extract_data(file_path):\n",
        "    \"\"\"\n",
        "    Extracts data from a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    file_path (str): The path to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: Loaded data as a Pandas DataFrame.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = pd.read_csv(file_path)\n",
        "        print(\"Data extracted successfully.\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error in data extraction: {e}\")\n",
        "        return None\n",
        "\n",
        "# Extracting data from the Titanic dataset\n",
        "file_path = \"datasets/titanic.csv\"\n",
        "data = extract_data(file_path)\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "6caj3fliMaDP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data transformed successfully.\n",
            "   PassengerId  Survived  Pclass             Name     Sex    Age     Fare  \\\n",
            "0            1         0       3         John Doe    male  22.00   7.2500   \n",
            "1            2         1       1       Jane Smith  female  38.00  71.2833   \n",
            "2            3         1       3      Alice Brown  female  30.75   8.0500   \n",
            "3            4         0       1  William Johnson    male  35.00  53.1000   \n",
            "4            5         1       3        Linda Lee  female  30.75   8.0500   \n",
            "\n",
            "  Age_Group  \n",
            "0     Adult  \n",
            "1     Adult  \n",
            "2     Adult  \n",
            "3     Adult  \n",
            "4     Adult  \n",
            "Data transformed successfully.\n",
            "   PassengerId  Survived  Pclass             Name     Sex    Age     Fare  \\\n",
            "0            1         0       3         John Doe    male  22.00   7.2500   \n",
            "1            2         1       1       Jane Smith  female  38.00  71.2833   \n",
            "2            3         1       3      Alice Brown  female  30.75   8.0500   \n",
            "3            4         0       1  William Johnson    male  35.00  53.1000   \n",
            "4            5         1       3        Linda Lee  female  30.75   8.0500   \n",
            "\n",
            "  Age_Group  \n",
            "0     Adult  \n",
            "1     Adult  \n",
            "2     Adult  \n",
            "3     Adult  \n",
            "4     Adult  \n"
          ]
        }
      ],
      "source": [
        "# Step 2 - Transform\n",
        "\n",
        "def transform_data(data):\n",
        "    \"\"\"\n",
        "    Transforms the data by handling missing values, removing duplicates,\n",
        "    and standardizing data formats.\n",
        "\n",
        "    Parameters:\n",
        "    data (DataFrame): The raw data to be transformed.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: Transformed data.\n",
        "    \"\"\"\n",
        "    # Make an explicit copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "    data = data.copy()\n",
        "\n",
        "    # Handling missing values\n",
        "    data['Age'] = data['Age'].fillna(data['Age'].mean())  # Fill missing ages with the mean age\n",
        "    data['Fare'] = data['Fare'].fillna(data['Fare'].median())  # Fill missing fares with the median fare\n",
        "\n",
        "    # Removing duplicates\n",
        "    data = data.drop_duplicates()\n",
        "\n",
        "    # Standardizing text format\n",
        "    data['Name'] = data['Name'].str.title()  # Capitalize names\n",
        "    data['Sex'] = data['Sex'].str.lower()    # Convert gender to lowercase\n",
        "\n",
        "    # Adding a new derived column: Age group\n",
        "    data['Age_Group'] = pd.cut(data['Age'], bins=[0, 12, 18, 50, 100], labels=['Child', 'Teen', 'Adult', 'Senior'])\n",
        "\n",
        "    print(\"Data transformed successfully.\")\n",
        "    return data\n",
        "\n",
        "# Transforming the extracted data\n",
        "transformed_data = transform_data(data)\n",
        "print(transformed_data.head())\n",
        "\n",
        "# Transforming the extracted data\n",
        "transformed_data = transform_data(data)\n",
        "print(transformed_data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "NMUbbm1yMjk_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully to datasets/titanic_transformed.csv.\n"
          ]
        }
      ],
      "source": [
        "# Step 3 - Load\n",
        "\n",
        "def load_data(data, output_file):\n",
        "    \"\"\"\n",
        "    Loads data into a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    data (DataFrame): The data to be saved.\n",
        "    output_file (str): The path where the file will be saved.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data.to_csv(output_file, index=False)\n",
        "        print(f\"Data loaded successfully to {output_file}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in data loading: {e}\")\n",
        "\n",
        "# Loading the transformed data to a new file\n",
        "output_file = \"datasets/titanic_transformed.csv\"\n",
        "load_data(transformed_data, output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "7LN2QHItMrl5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data extracted successfully.\n",
            "Data transformed successfully.\n",
            "Data loaded successfully to datasets/titanic_transformed.csv.\n"
          ]
        }
      ],
      "source": [
        "# Putting It All Together: Complete ETL Pipeline\n",
        "\n",
        "def etl_pipeline(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Runs the ETL process: extracts, transforms, and loads the data.\n",
        "\n",
        "    Parameters:\n",
        "    input_file (str): Path to the input CSV file.\n",
        "    output_file (str): Path for saving the transformed data.\n",
        "    \"\"\"\n",
        "    # Extract data\n",
        "    data = extract_data(input_file)\n",
        "\n",
        "    # If extraction was successful, proceed with transformation and loading\n",
        "    if data is not None:\n",
        "        # Transform data\n",
        "        transformed_data = transform_data(data)\n",
        "\n",
        "        # Load data\n",
        "        load_data(transformed_data, output_file)\n",
        "\n",
        "# Running the ETL pipeline\n",
        "input_file = \"datasets/titanic.csv\"\n",
        "output_file = \"datasets/titanic_transformed.csv\"\n",
        "etl_pipeline(input_file, output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SwYeBonPcBG"
      },
      "source": [
        "# 12. Data Quality, Testing, and Code Standards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "1dFEfyR_Mtxj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values in each column:\n",
            " PassengerId    0\n",
            "Survived       0\n",
            "Pclass         0\n",
            "Name           0\n",
            "Sex            0\n",
            "Age            3\n",
            "Fare           2\n",
            "dtype: int64\n",
            "\n",
            "Data types:\n",
            " PassengerId      int64\n",
            "Survived         int64\n",
            "Pclass           int64\n",
            "Name               str\n",
            "Sex                str\n",
            "Age            float64\n",
            "Fare           float64\n",
            "dtype: object\n",
            "\n",
            "Unique values in 'Sex' column: <ArrowStringArray>\n",
            "['male', 'female']\n",
            "Length: 2, dtype: str\n"
          ]
        }
      ],
      "source": [
        "# 12.1 Data Validation Techniques and Quality Checks\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Titanic dataset\n",
        "data = pd.read_csv(\"datasets/titanic.csv\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_data = data.isnull().sum()\n",
        "print(\"Missing values in each column:\\n\", missing_data)\n",
        "\n",
        "# Validate column data types\n",
        "print(\"\\nData types:\\n\", data.dtypes)\n",
        "\n",
        "# Validate unique values in categorical columns\n",
        "print(\"\\nUnique values in 'Sex' column:\", data['Sex'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "ZNTOevnsRdib"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.010s\n",
            "\n",
            "\u001b[32mOK\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data transformed successfully.\n"
          ]
        }
      ],
      "source": [
        "# 12.2 Writing Unit Tests for Data Pipelines Using Pythons unittest Framework\n",
        "\n",
        "import unittest\n",
        "import pandas as pd\n",
        "\n",
        "# Define the transform_data function directly in this script\n",
        "def transform_data(data):\n",
        "    \"\"\"\n",
        "    Transforms the data by handling missing values, removing duplicates,\n",
        "    and standardizing data formats.\n",
        "\n",
        "    Parameters:\n",
        "    data (DataFrame): The raw data to be transformed.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: Transformed data.\n",
        "    \"\"\"\n",
        "    # Handling missing values\n",
        "    data.loc[:, 'Age'] = data['Age'].fillna(data['Age'].mean())  # Fill missing ages with the mean age\n",
        "    data.loc[:, 'Fare'] = data['Fare'].fillna(data['Fare'].median())  # Fill missing fares with the median fare\n",
        "\n",
        "    # Removing duplicates\n",
        "    data = data.drop_duplicates()\n",
        "\n",
        "    # Standardizing text format\n",
        "    data.loc[:, 'Name'] = data['Name'].str.title()  # Capitalize names\n",
        "    data.loc[:, 'Sex'] = data['Sex'].str.lower()    # Convert gender to lowercase\n",
        "\n",
        "    # Adding a new derived column: Age group\n",
        "    data['Age_Group'] = pd.cut(data['Age'], bins=[0, 12, 18, 50, 100], labels=['Child', 'Teen', 'Adult', 'Senior'])\n",
        "\n",
        "    print(\"Data transformed successfully.\")\n",
        "    return data\n",
        "\n",
        "# Define the unit test for the transform_data function\n",
        "class TestTransformData(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"Set up a sample dataset with missing values and duplicates for testing.\"\"\"\n",
        "        self.data = pd.DataFrame({\n",
        "            \"PassengerId\": [1, 2, 3, 3, 5],\n",
        "            \"Survived\": [0, 1, 1, 1, 0],\n",
        "            \"Pclass\": [3, 1, 3, 3, 3],\n",
        "            \"Name\": [\"John Doe\", \"Jane Smith\", \"Alice Brown\", \"Alice Brown\", \"Linda Lee\"],\n",
        "            \"Sex\": [\"male\", \"female\", \"female\", \"female\", \"female\"],\n",
        "            \"Age\": [22, 38, None, None, 28],\n",
        "            \"Fare\": [7.25, 71.2833, 7.925, 7.925, None]\n",
        "        })\n",
        "\n",
        "    def test_transform_data(self):\n",
        "        \"\"\"Test the transform_data function for expected transformations.\"\"\"\n",
        "        transformed_data = transform_data(self.data.copy())\n",
        "\n",
        "        # Check missing values are filled\n",
        "        self.assertFalse(transformed_data['Age'].isnull().any(), \"Age column has missing values\")\n",
        "        self.assertFalse(transformed_data['Fare'].isnull().any(), \"Fare column has missing values\")\n",
        "\n",
        "        # Check for duplicates\n",
        "        self.assertEqual(len(transformed_data), len(transformed_data.drop_duplicates()), \"Duplicates not removed\")\n",
        "\n",
        "        # Check that 'Age_Group' column was created\n",
        "        self.assertIn(\"Age_Group\", transformed_data.columns, \"Age_Group column not created\")\n",
        "\n",
        "        # Check standardized formatting for text\n",
        "        self.assertTrue(transformed_data['Name'].str.istitle().all(), \"Names are not properly capitalized\")\n",
        "        self.assertTrue(transformed_data['Sex'].str.islower().all(), \"Sex column is not in lowercase\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(argv=[''], exit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "collapsed": true,
        "id": "EG3vxKexRiHQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting great_expectations\n",
            "  Downloading great_expectations-0.18.22-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting altair<5.0.0,>=4.2.1 (from great_expectations)\n",
            "  Downloading altair-4.2.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting Click>=7.1.2 (from great_expectations)\n",
            "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting colorama>=0.4.3 (from great_expectations)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting cryptography>=3.2 (from great_expectations)\n",
            "  Downloading cryptography-46.0.4-cp311-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: Ipython>=7.16.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from great_expectations) (9.9.0)\n",
            "Requirement already satisfied: ipywidgets>=7.5.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from great_expectations) (8.1.8)\n",
            "Requirement already satisfied: jinja2>=2.10 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from great_expectations) (3.1.6)\n",
            "Collecting jsonpatch>=1.22 (from great_expectations)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: jsonschema>=2.5.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from great_expectations) (4.26.0)\n",
            "Collecting makefun<2,>=1.7.0 (from great_expectations)\n",
            "  Downloading makefun-1.16.0-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.7.1 (from great_expectations)\n",
            "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: mistune>=0.8.4 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from great_expectations) (3.2.0)\n",
            "Requirement already satisfied: nbformat>=5.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from great_expectations) (5.10.4)\n",
            "Requirement already satisfied: notebook>=6.4.10 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from great_expectations) (7.5.2)\n",
            "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from great_expectations) (26.0)\n",
            "Collecting pydantic>=1.9.2 (from great_expectations)\n",
            "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
            "Requirement already satisfied: pyparsing>=2.4 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from great_expectations) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from great_expectations) (2.9.0.post0)\n",
            "Collecting pytz>=2021.3 (from great_expectations)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from great_expectations) (2.32.5)\n",
            "Collecting ruamel.yaml<0.18,>=0.16 (from great_expectations)\n",
            "  Downloading ruamel.yaml-0.17.40-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from great_expectations) (1.17.0)\n",
            "Collecting tqdm>=4.59.0 (from great_expectations)\n",
            "  Downloading tqdm-4.67.2-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from great_expectations) (4.15.0)\n",
            "Collecting tzlocal>=1.2 (from great_expectations)\n",
            "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: urllib3>=1.26 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from great_expectations) (2.6.3)\n",
            "Collecting numpy<2.0.0,>=1.22.4 (from great_expectations)\n",
            "  Downloading numpy-1.26.4.tar.gz (15.8 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from great_expectations) (3.0.0)\n",
            "Collecting entrypoints (from altair<5.0.0,>=4.2.1->great_expectations)\n",
            "  Downloading entrypoints-0.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting toolz (from altair<5.0.0,>=4.2.1->great_expectations)\n",
            "  Downloading toolz-1.1.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: cffi>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from cryptography>=3.2->great_expectations) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from cffi>=2.0.0->cryptography>=3.2->great_expectations) (3.0)\n",
            "Requirement already satisfied: decorator>=4.3.2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from Ipython>=7.16.3->great_expectations) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from Ipython>=7.16.3->great_expectations) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.18.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from Ipython>=7.16.3->great_expectations) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1.5 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from Ipython>=7.16.3->great_expectations) (0.2.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from Ipython>=7.16.3->great_expectations) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from Ipython>=7.16.3->great_expectations) (3.0.52)\n",
            "Requirement already satisfied: pygments>=2.11.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from Ipython>=7.16.3->great_expectations) (2.19.2)\n",
            "Requirement already satisfied: stack_data>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from Ipython>=7.16.3->great_expectations) (0.6.3)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from Ipython>=7.16.3->great_expectations) (5.14.3)\n",
            "Requirement already satisfied: wcwidth in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->Ipython>=7.16.3->great_expectations) (0.3.5)\n",
            "Requirement already satisfied: comm>=0.1.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from ipywidgets>=7.5.1->great_expectations) (0.2.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from ipywidgets>=7.5.1->great_expectations) (4.0.15)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from ipywidgets>=7.5.1->great_expectations) (3.0.16)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jedi>=0.18.1->Ipython>=7.16.3->great_expectations) (0.8.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jinja2>=2.10->great_expectations) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jsonpatch>=1.22->great_expectations) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jsonschema>=2.5.1->great_expectations) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jsonschema>=2.5.1->great_expectations) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jsonschema>=2.5.1->great_expectations) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jsonschema>=2.5.1->great_expectations) (0.30.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from nbformat>=5.0->great_expectations) (2.21.2)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from nbformat>=5.0->great_expectations) (5.9.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.0->great_expectations) (4.5.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from notebook>=6.4.10->great_expectations) (2.17.0)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.28.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from notebook>=6.4.10->great_expectations) (2.28.0)\n",
            "Requirement already satisfied: jupyterlab<4.6,>=4.5.2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from notebook>=6.4.10->great_expectations) (4.5.3)\n",
            "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from notebook>=6.4.10->great_expectations) (0.2.4)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from notebook>=6.4.10->great_expectations) (6.5.4)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (4.12.1)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (25.1.0)\n",
            "Requirement already satisfied: jupyter-client>=7.4.4 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (8.8.0)\n",
            "Requirement already satisfied: jupyter-events>=0.11.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (0.5.4)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (7.16.6)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (0.24.1)\n",
            "Requirement already satisfied: pyzmq>=24 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (27.1.0)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (2.1.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (1.9.0)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyterlab<4.6,>=4.5.2->notebook>=6.4.10->great_expectations) (2.1.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyterlab<4.6,>=4.5.2->notebook>=6.4.10->great_expectations) (0.28.1)\n",
            "Requirement already satisfied: ipykernel!=6.30.0,>=6.5.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyterlab<4.6,>=4.5.2->notebook>=6.4.10->great_expectations) (7.1.0)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyterlab<4.6,>=4.5.2->notebook>=6.4.10->great_expectations) (2.3.0)\n",
            "Requirement already satisfied: setuptools>=41.1.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyterlab<4.6,>=4.5.2->notebook>=6.4.10->great_expectations) (80.10.1)\n",
            "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from httpx<1,>=0.25.0->jupyterlab<4.6,>=4.5.2->notebook>=6.4.10->great_expectations) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from httpx<1,>=0.25.0->jupyterlab<4.6,>=4.5.2->notebook>=6.4.10->great_expectations) (1.0.9)\n",
            "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from httpx<1,>=0.25.0->jupyterlab<4.6,>=4.5.2->notebook>=6.4.10->great_expectations) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab<4.6,>=4.5.2->notebook>=6.4.10->great_expectations) (0.16.0)\n",
            "Requirement already satisfied: babel>=2.10 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyterlab-server<3,>=2.28.0->notebook>=6.4.10->great_expectations) (2.17.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyterlab-server<3,>=2.28.0->notebook>=6.4.10->great_expectations) (0.13.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (25.1.0)\n",
            "Requirement already satisfied: appnope>=0.1.2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.2->notebook>=6.4.10->great_expectations) (0.1.4)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.2->notebook>=6.4.10->great_expectations) (1.8.19)\n",
            "Requirement already satisfied: nest-asyncio>=1.4 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.2->notebook>=6.4.10->great_expectations) (1.6.0)\n",
            "Requirement already satisfied: psutil>=5.7 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.2->notebook>=6.4.10->great_expectations) (7.2.1)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (4.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (6.0.3)\n",
            "Requirement already satisfied: rfc3339-validator in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (0.1.1)\n",
            "Requirement already satisfied: fqdn in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (20.11.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (25.10.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (4.14.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (0.3.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (0.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (1.5.1)\n",
            "Requirement already satisfied: webencodings in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (1.4.0)\n",
            "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pandas>=1.3.0 (from great_expectations)\n",
            "  Downloading pandas-2.3.3-cp314-cp314-macosx_11_0_arm64.whl.metadata (91 kB)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from pandas>=1.3.0->great_expectations) (2025.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from pexpect>4.3->Ipython>=7.16.3->great_expectations) (0.7.0)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic>=1.9.2->great_expectations)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.41.5 (from pydantic>=1.9.2->great_expectations)\n",
            "  Downloading pydantic_core-2.41.5-cp314-cp314-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspection>=0.4.2 (from pydantic>=1.9.2->great_expectations)\n",
            "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from python-dateutil>=2.8.1->great_expectations) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from requests>=2.20->great_expectations) (3.4.4)\n",
            "Requirement already satisfied: lark>=1.2.2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (1.3.1)\n",
            "Requirement already satisfied: executing>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from stack_data>=0.6.0->Ipython>=7.16.3->great_expectations) (2.2.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from stack_data>=0.6.0->Ipython>=7.16.3->great_expectations) (3.0.1)\n",
            "Requirement already satisfied: pure-eval in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from stack_data>=0.6.0->Ipython>=7.16.3->great_expectations) (0.2.3)\n",
            "Requirement already satisfied: soupsieve>=1.6.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (2.8.3)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great_expectations) (1.4.0)\n",
            "Downloading great_expectations-0.18.22-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m813.6/813.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading makefun-1.16.0-py2.py3-none-any.whl (22 kB)\n",
            "Downloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "Downloading ruamel.yaml-0.17.40-py3-none-any.whl (113 kB)\n",
            "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading cryptography-46.0.4-cp311-abi3-macosx_10_9_universal2.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading pandas-2.3.3-cp314-cp314-macosx_11_0_arm64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
            "Downloading pydantic_core-2.41.5-cp314-cp314-macosx_11_0_arm64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tqdm-4.67.2-py3-none-any.whl (78 kB)\n",
            "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
            "Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
            "Downloading toolz-1.1.0-py3-none-any.whl (58 kB)\n",
            "Building wheels for collected packages: numpy\n",
            "  Building wheel for numpy (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for numpy: filename=numpy-1.26.4-cp314-cp314-macosx_15_0_arm64.whl size=4738095 sha256=af4d9ddaf26976aaa8824f4144c72cafa435b5746b701a9a77213998725fc8ae\n",
            "  Stored in directory: /Users/rifqialanmaulana/Library/Caches/pip/wheels/9f/41/2d/41d9edbbdf1331835527392f3da54c7de611dc6ba3ef296215\n",
            "Successfully built numpy\n",
            "Installing collected packages: pytz, makefun, tzlocal, typing-inspection, tqdm, toolz, ruamel.yaml, pydantic-core, numpy, marshmallow, jsonpatch, entrypoints, colorama, Click, annotated-types, pydantic, pandas, cryptography, altair, great_expectations\n",
            "\u001b[2K  Attempting uninstall: numpy\u001b[0m\u001b[90m\u001b[0m \u001b[32m 6/20\u001b[0m [ruamel.yaml]\n",
            "\u001b[2K    Found existing installation: numpy 2.4.1\u001b[0m \u001b[32m 6/20\u001b[0m [ruamel.yaml]\n",
            "\u001b[2K    Uninstalling numpy-2.4.1:m\u001b[90m\u001b[0m \u001b[32m 6/20\u001b[0m [ruamel.yaml]\n",
            "\u001b[2K      Successfully uninstalled numpy-2.4.1\u001b[0m \u001b[32m 6/20\u001b[0m [ruamel.yaml]\n",
            "\u001b[2K  Attempting uninstall: pandas\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m15/20\u001b[0m [pydantic]\n",
            "\u001b[2K    Found existing installation: pandas 3.0.0[0m\u001b[90m\u001b[0m \u001b[32m15/20\u001b[0m [pydantic]\n",
            "\u001b[2K    Uninstalling pandas-3.0.0:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m15/20\u001b[0m [pydantic]\n",
            "\u001b[2K      Successfully uninstalled pandas-3.0.0\u001b[0m\u001b[90m\u001b[0m \u001b[32m15/20\u001b[0m [pydantic]\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m20/20\u001b[0m [great_expectations]reat_expectations]\n",
            "\u001b[1A\u001b[2KSuccessfully installed Click-8.3.1 altair-4.2.2 annotated-types-0.7.0 colorama-0.4.6 cryptography-46.0.4 entrypoints-0.4 great_expectations-0.18.22 jsonpatch-1.33 makefun-1.16.0 marshmallow-3.26.2 numpy-1.26.4 pandas-2.3.3 pydantic-2.12.5 pydantic-core-2.41.5 pytz-2025.2 ruamel.yaml-0.17.40 toolz-1.1.0 tqdm-4.67.2 typing-inspection-0.4.2 tzlocal-5.3.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# 12.3 Advanced Data Quality Checks with great_expectations\n",
        "\n",
        "!pip3 install great_expectations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "Fzpf6ytzSFdi"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'numpy' has no attribute '_no_nep50_warning'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgx\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Initialize the Great Expectations context\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/great_expectations/__init__.py:6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_versions  \u001b[38;5;66;03m# isort:skip\u001b[39;00m\n\u001b[32m      4\u001b[39m __version__ = get_versions()[\u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# isort:skip\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_context\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmigrator\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcloud_migrator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CloudMigrator\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexpectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_core_expectations\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m get_versions  \u001b[38;5;66;03m# isort:skip\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/great_expectations/data_context/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_context\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_context\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     AbstractDataContext,\n\u001b[32m      3\u001b[39m     BaseDataContext,\n\u001b[32m      4\u001b[39m     CloudDataContext,\n\u001b[32m      5\u001b[39m     DataContext,\n\u001b[32m      6\u001b[39m     EphemeralDataContext,\n\u001b[32m      7\u001b[39m     FileDataContext,\n\u001b[32m      8\u001b[39m     get_context,\n\u001b[32m      9\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/great_expectations/data_context/data_context/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_context\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_context\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mabstract_data_context\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     AbstractDataContext,\n\u001b[32m      3\u001b[39m )\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_context\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_context\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase_data_context\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     BaseDataContext,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_context\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_context\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcloud_data_context\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     CloudDataContext,\n\u001b[32m      9\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/great_expectations/data_context/data_context/abstract_data_context.py:61\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_peer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConfigPeer\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_provider\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     56\u001b[39m     _ConfigurationProvider,\n\u001b[32m     57\u001b[39m     _ConfigurationVariablesConfigurationProvider,\n\u001b[32m     58\u001b[39m     _EnvironmentConfigurationProvider,\n\u001b[32m     59\u001b[39m     _RuntimeEnvironmentConfigurationProvider,\n\u001b[32m     60\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasource_dict\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CacheableDatasourceDict\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexpectation_validation_result\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_metric_kwargs_id\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mid_dict\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchKwargs\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/great_expectations/core/datasource_dict.py:13\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompatibility\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_context\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     DatasourceConfig,\n\u001b[32m     11\u001b[39m     datasourceConfigSchema,\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasource\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfluent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Datasource \u001b[38;5;28;01mas\u001b[39;00m FluentDatasource\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasource\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfluent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _IN_MEMORY_DATA_ASSET_TYPE\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/great_expectations/datasource/__init__.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_datasource\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PandasDatasource\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msimple_sqlalchemy_datasource\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleSqlalchemyDatasource\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparkdf_datasource\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkDFDatasource\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/great_expectations/datasource/sparkdf_datasource.py:7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompatibility\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pyspark\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Batch, BatchMarkers\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkDFDataset\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasource\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasource\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LegacyDatasource\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchKwargsError\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/great_expectations/dataset/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MetaPandasDataset, PandasDataset\n\u001b[32m      6\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/great_expectations/dataset/dataset.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdateutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompatibility\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sqlalchemy\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgreat_expectations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_asset\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_asset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataAsset\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/scipy/stats/__init__.py:628\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m \n\u001b[32m    624\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_warnings_errors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[32m    627\u001b[39m                                DegenerateDataWarning, FitError)\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stats_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_variation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/scipy/stats/_stats_py.py:39\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array, asarray, ma\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspatial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distance_matrix\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m milp, LinearConstraint\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/scipy/sparse/__init__.py:307\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_warnings\u001b[39;00m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_importlib\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/scipy/sparse/_base.py:8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moperator\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[32m      9\u001b[39m                        get_sum_dtype, isdense, isscalarlike, _todata,\n\u001b[32m     10\u001b[39m                        matrix, validateaxis, getdtype, is_pydata_spmatrix)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseABC, issparse\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/scipy/sparse/_sputils.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prod\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m np_long, np_ulong\n\u001b[32m     13\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mupcast\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgetdtype\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgetdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misscalarlike\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misintlike\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     14\u001b[39m            \u001b[33m'\u001b[39m\u001b[33misshape\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33missequence\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misdense\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mismatrix\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mget_sum_dtype\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     15\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mbroadcast_shapes\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     17\u001b[39m supported_dtypes = [np.bool_, np.byte, np.ubyte, np.short, np.ushort, np.intc,\n\u001b[32m     18\u001b[39m                     np.uintc, np_long, np_ulong, np.longlong, np.ulonglong,\n\u001b[32m     19\u001b[39m                     np.float32, np.float64, np.longdouble,\n\u001b[32m     20\u001b[39m                     np.complex64, np.complex128, np.clongdouble]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/scipy/_lib/_util.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Literal, TypeAlias, TypeVar\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (Array, array_namespace, is_lazy_array, is_numpy,\n\u001b[32m     18\u001b[39m                                    is_marray, xp_size, xp_result_device, xp_result_type)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_docscrape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionDoc, Parameter\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m issparse\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/scipy/_lib/_array_api.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnpt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     is_array_api_obj,\n\u001b[32m     26\u001b[39m     is_lazy_array,\n\u001b[32m     27\u001b[39m     is_numpy_array,\n\u001b[32m     28\u001b[39m     is_cupy_array,\n\u001b[32m     29\u001b[39m     is_torch_array,\n\u001b[32m     30\u001b[39m     is_jax_array,\n\u001b[32m     31\u001b[39m     is_dask_array,\n\u001b[32m     32\u001b[39m     size \u001b[38;5;28;01mas\u001b[39;00m xp_size,\n\u001b[32m     33\u001b[39m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[32m     34\u001b[39m     device \u001b[38;5;28;01mas\u001b[39;00m xp_device,\n\u001b[32m     35\u001b[39m     is_numpy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_numpy,\n\u001b[32m     36\u001b[39m     is_cupy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_cupy,\n\u001b[32m     37\u001b[39m     is_torch_namespace \u001b[38;5;28;01mas\u001b[39;00m is_torch,\n\u001b[32m     38\u001b[39m     is_jax_namespace \u001b[38;5;28;01mas\u001b[39;00m is_jax,\n\u001b[32m     39\u001b[39m     is_dask_namespace \u001b[38;5;28;01mas\u001b[39;00m is_dask,\n\u001b[32m     40\u001b[39m     is_array_api_strict_namespace \u001b[38;5;28;01mas\u001b[39;00m is_array_api_strict,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_helpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _compat_module_name\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_extra\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtesting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lazy_xp_function\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/scipy/_lib/array_api_compat/numpy/__init__.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# This needs to be loaded explicitly before cloning\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m __all__ = \u001b[43mclone_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnumpy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# These imports may overwrite names from the import * above.\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _aliases\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/scipy/_lib/array_api_compat/_internal.py:64\u001b[39m, in \u001b[36mclone_module\u001b[39m\u001b[34m(mod_name, globals_)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Neither of these two methods is sufficient by itself,\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# depending on various idiosyncrasies of the libraries we're wrapping.\u001b[39;00m\n\u001b[32m     63\u001b[39m objs = {}\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrom \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m import *\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(mod):\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(mod, n):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/numpy/testing/__init__.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munittest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TestCase\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _private\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_private\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_private\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (_assert_valid_refcount, _gen_alignment_data)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_private\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extbuild\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/numpy/testing/_private/utils.py:413\u001b[39m\n\u001b[32m    409\u001b[39m         pprint.pprint(desired, msg)\n\u001b[32m    410\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg.getvalue())\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m \u001b[38;5;129m@np\u001b[39m\u001b[43m.\u001b[49m\u001b[43m_no_nep50_warning\u001b[49m()\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34massert_almost_equal\u001b[39m(actual, desired, decimal=\u001b[32m7\u001b[39m, err_msg=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    415\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[33;03m    Raises an AssertionError if two items are not equal up to desired\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m    precision.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    481\u001b[39m \n\u001b[32m    482\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    483\u001b[39m     __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Hide traceback for py.test\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/numpy/__init__.py:792\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
            "\u001b[31mAttributeError\u001b[39m: module 'numpy' has no attribute '_no_nep50_warning'"
          ]
        }
      ],
      "source": [
        "import great_expectations as gx\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the Great Expectations context\n",
        "context = gx.get_context()\n",
        "\n",
        "# Step 1: Load the Titanic dataset as a Pandas DataFrame\n",
        "dataframe = pd.read_csv(\"datasets/titanic.csv\")\n",
        "\n",
        "# Step 2: Define Data Source Parameters\n",
        "data_source_name = \"my_data_source\"\n",
        "data_source = context.data_sources.add_pandas(name=data_source_name)\n",
        "\n",
        "# Step 3: Define Data Asset\n",
        "data_asset_name = \"my_dataframe_data_asset\"\n",
        "data_asset = data_source.add_dataframe_asset(name=data_asset_name)\n",
        "\n",
        "# Step 4: Define Batch Parameters with the loaded DataFrame\n",
        "batch_parameters = {\"dataframe\": dataframe}\n",
        "\n",
        "# Step 5: Create Batch Definition and get a Batch of Data\n",
        "batch_definition_name = \"my_batch_definition\"\n",
        "batch_definition = data_asset.add_batch_definition_whole_dataframe(batch_definition_name)\n",
        "batch = batch_definition.get_batch(batch_parameters=batch_parameters)\n",
        "\n",
        "# Step 6: Define and Run Expectations\n",
        "expectations = [\n",
        "    gx.expectations.ExpectColumnValuesToNotBeNull(column=\"PassengerId\"),\n",
        "    gx.expectations.ExpectColumnValuesToBeUnique(column=\"PassengerId\"),\n",
        "    gx.expectations.ExpectColumnValuesToNotBeNull(column=\"Age\"),\n",
        "    gx.expectations.ExpectColumnValuesToBeInSet(column=\"Sex\", value_set=[\"male\", \"female\"]),\n",
        "    gx.expectations.ExpectColumnValuesToBeBetween(column=\"Fare\", min_value=0, max_value=500)\n",
        "]\n",
        "\n",
        "# Step 7: Run Validations\n",
        "validation_results = []\n",
        "for expectation in expectations:\n",
        "    validation_results.append(batch.validate(expectation))\n",
        "\n",
        "# Display the validation results\n",
        "for i, result in enumerate(validation_results, start=1):\n",
        "    print(f\"Validation {i}: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViHhfao6SKAb",
        "outputId": "7a73dc36-d1c0-4577-afbb-39ec19b3ee31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flake8 in /usr/local/lib/python3.10/dist-packages (7.1.1)\n",
            "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from flake8) (0.7.0)\n",
            "Requirement already satisfied: pycodestyle<2.13.0,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from flake8) (2.12.1)\n",
            "Requirement already satisfied: pyflakes<3.3.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from flake8) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "# 12.4 - Static Code Analysis and Adherence to PEP 8 Standards\n",
        "\n",
        "!pip install flake8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "58c9CD2bFc04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files 'non_pep8_script.py' and 'pep8_script.py' created successfully.\n"
          ]
        }
      ],
      "source": [
        "# Flake 8 Setup -\n",
        "\n",
        "# Code to create non_pep8_script.py\n",
        "non_pep8_code = \"\"\"import pandas as pd, numpy as np # Multiple imports on one line\n",
        "\n",
        "def Load_Data(filepath): # Function name should be lowercase with underscores\n",
        "    data=pd.read_csv(filepath) # No spaces around '='\n",
        "    print(\"Data loaded:\",data.head()) # No space after the comma\n",
        "    return data\n",
        "\n",
        "def Process_Data(data): # Function name should be lowercase with underscores\n",
        "    data['new_column'] = data['Age']* 2 # Inconsistent spacing around operators\n",
        "    print(\"New column added\") # Missing blank line above function\n",
        "    return data\n",
        "\n",
        "def main(): # Missing two blank lines above function definition\n",
        "    df=Load_Data(\"datasets/titanic.csv\") # No spaces around '='\n",
        "    result=Process_Data(df) # No spaces around '='\n",
        "    print(\"Processing Complete\") # Missing newline at end of file\n",
        "\n",
        "main()\n",
        "\"\"\"\n",
        "\n",
        "# Code to create pep8_script.py\n",
        "pep8_code = \"\"\"import pandas as pd  # 'numpy as np' removed since it is unused\n",
        "\n",
        "\n",
        "def load_data(filepath):\n",
        "    \\\"\"\"\n",
        "    Loads data from a CSV file.\n",
        "    \\\"\"\"\n",
        "    data = pd.read_csv(filepath)\n",
        "    print(\"Data loaded:\", data.head())\n",
        "    return data\n",
        "\n",
        "\n",
        "def process_data(data):\n",
        "    \\\"\"\"\n",
        "    Adds a new column to the data based on the Age column.\n",
        "    \\\"\"\"\n",
        "    data['new_column'] = data['Age'] * 2\n",
        "    print(\"New column added\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def main():\n",
        "    \\\"\"\"\n",
        "    Main function to load, process, and display the data.\n",
        "    \\\"\"\"\n",
        "    df = load_data(\"datasets/titanic.csv\")\n",
        "    df = process_data(df)  # Removed unused 'result' variable\n",
        "    print(\"Processing Complete\")\n",
        "\n",
        "\n",
        "main()\n",
        "\"\"\"\n",
        "\n",
        "# Writing the scripts to files\n",
        "with open(\"non_pep8_script.py\", \"w\") as non_pep8_file:\n",
        "    non_pep8_file.write(non_pep8_code)\n",
        "\n",
        "with open(\"pep8_script.py\", \"w\") as pep8_file:\n",
        "    pep8_file.write(pep8_code)\n",
        "\n",
        "print(\"Files 'non_pep8_script.py' and 'pep8_script.py' created successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5HEM7BKUAmC",
        "outputId": "616d98f6-5c83-4641-f4e5-51401543b223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m1\u001b[36m:\u001b[m1\u001b[36m:\u001b[m \u001b[1m\u001b[31mF401\u001b[m 'numpy as np' imported but unused\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m1\u001b[36m:\u001b[m20\u001b[36m:\u001b[m \u001b[1m\u001b[31mE401\u001b[m multiple imports on one line\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m1\u001b[36m:\u001b[m33\u001b[36m:\u001b[m \u001b[1m\u001b[31mE261\u001b[m at least two spaces before inline comment\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m3\u001b[36m:\u001b[m1\u001b[36m:\u001b[m \u001b[1m\u001b[31mE302\u001b[m expected 2 blank lines, found 1\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m3\u001b[36m:\u001b[m25\u001b[36m:\u001b[m \u001b[1m\u001b[31mE261\u001b[m at least two spaces before inline comment\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m4\u001b[36m:\u001b[m9\u001b[36m:\u001b[m \u001b[1m\u001b[31mE225\u001b[m missing whitespace around operator\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m4\u001b[36m:\u001b[m31\u001b[36m:\u001b[m \u001b[1m\u001b[31mE261\u001b[m at least two spaces before inline comment\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m5\u001b[36m:\u001b[m25\u001b[36m:\u001b[m \u001b[1m\u001b[31mE231\u001b[m missing whitespace after ','\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m5\u001b[36m:\u001b[m38\u001b[36m:\u001b[m \u001b[1m\u001b[31mE261\u001b[m at least two spaces before inline comment\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m8\u001b[36m:\u001b[m1\u001b[36m:\u001b[m \u001b[1m\u001b[31mE302\u001b[m expected 2 blank lines, found 1\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m8\u001b[36m:\u001b[m24\u001b[36m:\u001b[m \u001b[1m\u001b[31mE261\u001b[m at least two spaces before inline comment\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m9\u001b[36m:\u001b[m37\u001b[36m:\u001b[m \u001b[1m\u001b[31mE225\u001b[m missing whitespace around operator\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m9\u001b[36m:\u001b[m40\u001b[36m:\u001b[m \u001b[1m\u001b[31mE261\u001b[m at least two spaces before inline comment\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m10\u001b[36m:\u001b[m30\u001b[36m:\u001b[m \u001b[1m\u001b[31mE261\u001b[m at least two spaces before inline comment\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m13\u001b[36m:\u001b[m1\u001b[36m:\u001b[m \u001b[1m\u001b[31mE302\u001b[m expected 2 blank lines, found 1\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m13\u001b[36m:\u001b[m12\u001b[36m:\u001b[m \u001b[1m\u001b[31mE261\u001b[m at least two spaces before inline comment\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m14\u001b[36m:\u001b[m7\u001b[36m:\u001b[m \u001b[1m\u001b[31mE225\u001b[m missing whitespace around operator\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m14\u001b[36m:\u001b[m41\u001b[36m:\u001b[m \u001b[1m\u001b[31mE261\u001b[m at least two spaces before inline comment\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m15\u001b[36m:\u001b[m5\u001b[36m:\u001b[m \u001b[1m\u001b[31mF841\u001b[m local variable 'result' is assigned to but never used\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m15\u001b[36m:\u001b[m11\u001b[36m:\u001b[m \u001b[1m\u001b[31mE225\u001b[m missing whitespace around operator\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m15\u001b[36m:\u001b[m28\u001b[36m:\u001b[m \u001b[1m\u001b[31mE261\u001b[m at least two spaces before inline comment\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m16\u001b[36m:\u001b[m33\u001b[36m:\u001b[m \u001b[1m\u001b[31mE261\u001b[m at least two spaces before inline comment\n",
            "\u001b[1mnon_pep8_script.py\u001b[m\u001b[36m:\u001b[m18\u001b[36m:\u001b[m1\u001b[36m:\u001b[m \u001b[1m\u001b[31mE305\u001b[m expected 2 blank lines after class or function definition, found 1\n"
          ]
        }
      ],
      "source": [
        "!flake8 non_pep8_script.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUP99U-lFfbO"
      },
      "outputs": [],
      "source": [
        "!flake8 pep8_script.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liMmcxJNUaoc"
      },
      "source": [
        "# Section 13. Building Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "ZpQ9QDda-PkN"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[Errno 30] Read-only file system: '/content'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m package_dir = os.path.join(base_dir, package_name)\n\u001b[32m     12\u001b[39m tests_dir = os.path.join(base_dir, \u001b[33m\"\u001b[39m\u001b[33mtests\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpackage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m os.makedirs(tests_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Create __init__.py in the package and tests directories\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:226\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists(head):\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[32m    228\u001b[39m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[32m    229\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:226\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists(head):\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[32m    228\u001b[39m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[32m    229\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:236\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n\u001b[32m    234\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m    238\u001b[39m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.isdir(name):\n",
            "\u001b[31mOSError\u001b[39m: [Errno 30] Read-only file system: '/content'"
          ]
        }
      ],
      "source": [
        "# Section 13 Setup\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the package name and module names\n",
        "package_name = \"data_quality_analytics_vector\"\n",
        "modules = [\"etl\", \"quality_checks\"]\n",
        "\n",
        "# Create the base directory for the package\n",
        "base_dir = os.path.join(\"/content\", package_name)\n",
        "package_dir = os.path.join(base_dir, package_name)\n",
        "tests_dir = os.path.join(base_dir, \"tests\")\n",
        "os.makedirs(package_dir, exist_ok=True)\n",
        "os.makedirs(tests_dir, exist_ok=True)\n",
        "\n",
        "# Create __init__.py in the package and tests directories\n",
        "for init_dir in [package_dir, tests_dir]:\n",
        "    with open(os.path.join(init_dir, \"__init__.py\"), \"w\") as f:\n",
        "        f.write(\"# Initialization file\\n\")\n",
        "\n",
        "# Create module files inside the package\n",
        "for module in modules:\n",
        "    module_path = os.path.join(package_dir, f\"{module}.py\")\n",
        "    with open(module_path, \"w\") as f:\n",
        "        if module == \"etl\":\n",
        "            f.write(\"\"\"\n",
        "# ETL module for data quality analytics\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def load_data(filepath):\n",
        "    return pd.read_csv(filepath)\n",
        "\n",
        "def transform_data(df):\n",
        "    df = df.copy()\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df.fillna(method=\"ffill\", inplace=True)\n",
        "    return df\n",
        "\n",
        "def save_data(df, filepath):\n",
        "    df.to_csv(filepath, index=False)\n",
        "\"\"\")\n",
        "        elif module == \"quality_checks\":\n",
        "            f.write(\"\"\"\n",
        "# Quality checks module for data quality analytics\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def check_missing_values(df):\n",
        "    return df.isnull().sum()\n",
        "\n",
        "def check_duplicates(df):\n",
        "    return df.duplicated().sum()\n",
        "\"\"\")\n",
        "\n",
        "# Create an empty test file in the tests directory\n",
        "test_file_path = os.path.join(tests_dir, \"test_etl.py\")\n",
        "with open(test_file_path, \"w\") as f:\n",
        "    f.write(\"# Placeholder for ETL module tests\\n\")\n",
        "\n",
        "# Create setup.py for the package\n",
        "setup_content = f\"\"\"\n",
        "from setuptools import setup, find_packages\n",
        "\n",
        "setup(\n",
        "    name=\"{package_name}\",\n",
        "    version=\"0.1.0\",\n",
        "    description=\"A package for ETL and data quality checks.\",\n",
        "    author=\"Your Name\",\n",
        "    packages=find_packages(),\n",
        "    install_requires=[\n",
        "        \"pandas>=1.0.0\",\n",
        "    ],\n",
        "    python_requires=\">=3.6\",\n",
        ")\n",
        "\"\"\"\n",
        "with open(os.path.join(base_dir, \"setup.py\"), \"w\") as f:\n",
        "    f.write(setup_content)\n",
        "\n",
        "# Create README.md\n",
        "readme_content = f\"\"\"\\\n",
        "# {package_name}\n",
        "\n",
        "A Python package for ETL and data quality checks.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeYiMP8dWITo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/data_quality_analytics_vector\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJoNRBwwWNgz",
        "outputId": "607b79ca-69a8-4a32-a74b-3968c3010653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data_quality_analytics_vector  setup.py  tests\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbZeAgfOVGF7",
        "outputId": "6df8529b-af92-4fa6-9ca2-6343c6bc9a47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting build\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build) (24.2)\n",
            "Collecting pyproject_hooks (from build)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build) (2.1.0)\n",
            "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: pyproject_hooks, build\n",
            "Successfully installed build-1.2.2.post1 pyproject_hooks-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BQvbXGljVGIa",
        "outputId": "cc26050c-bcea-4bd3-ff49-31c6c1788397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m* Creating isolated environment: venv+pip...\u001b[0m\n",
            "\u001b[1m* Installing packages in isolated environment:\u001b[0m\n",
            "  - setuptools >= 40.8.0\n",
            "\u001b[1m* Getting build dependencies for sdist...\u001b[0m\n",
            "running egg_info\n",
            "creating data_quality_analytics_vector.egg-info\n",
            "writing data_quality_analytics_vector.egg-info/PKG-INFO\n",
            "writing dependency_links to data_quality_analytics_vector.egg-info/dependency_links.txt\n",
            "writing requirements to data_quality_analytics_vector.egg-info/requires.txt\n",
            "writing top-level names to data_quality_analytics_vector.egg-info/top_level.txt\n",
            "writing manifest file 'data_quality_analytics_vector.egg-info/SOURCES.txt'\n",
            "reading manifest file 'data_quality_analytics_vector.egg-info/SOURCES.txt'\n",
            "writing manifest file 'data_quality_analytics_vector.egg-info/SOURCES.txt'\n",
            "\u001b[1m* Building sdist...\u001b[0m\n",
            "running sdist\n",
            "running egg_info\n",
            "writing data_quality_analytics_vector.egg-info/PKG-INFO\n",
            "writing dependency_links to data_quality_analytics_vector.egg-info/dependency_links.txt\n",
            "writing requirements to data_quality_analytics_vector.egg-info/requires.txt\n",
            "writing top-level names to data_quality_analytics_vector.egg-info/top_level.txt\n",
            "reading manifest file 'data_quality_analytics_vector.egg-info/SOURCES.txt'\n",
            "writing manifest file 'data_quality_analytics_vector.egg-info/SOURCES.txt'\n",
            "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
            "\n",
            "running check\n",
            "creating data_quality_analytics_vector-0.1.0\n",
            "creating data_quality_analytics_vector-0.1.0/data_quality_analytics_vector\n",
            "creating data_quality_analytics_vector-0.1.0/data_quality_analytics_vector.egg-info\n",
            "creating data_quality_analytics_vector-0.1.0/tests\n",
            "copying files to data_quality_analytics_vector-0.1.0...\n",
            "copying setup.py -> data_quality_analytics_vector-0.1.0\n",
            "copying data_quality_analytics_vector/__init__.py -> data_quality_analytics_vector-0.1.0/data_quality_analytics_vector\n",
            "copying data_quality_analytics_vector/etl.py -> data_quality_analytics_vector-0.1.0/data_quality_analytics_vector\n",
            "copying data_quality_analytics_vector/quality_checks.py -> data_quality_analytics_vector-0.1.0/data_quality_analytics_vector\n",
            "copying data_quality_analytics_vector.egg-info/PKG-INFO -> data_quality_analytics_vector-0.1.0/data_quality_analytics_vector.egg-info\n",
            "copying data_quality_analytics_vector.egg-info/SOURCES.txt -> data_quality_analytics_vector-0.1.0/data_quality_analytics_vector.egg-info\n",
            "copying data_quality_analytics_vector.egg-info/dependency_links.txt -> data_quality_analytics_vector-0.1.0/data_quality_analytics_vector.egg-info\n",
            "copying data_quality_analytics_vector.egg-info/requires.txt -> data_quality_analytics_vector-0.1.0/data_quality_analytics_vector.egg-info\n",
            "copying data_quality_analytics_vector.egg-info/top_level.txt -> data_quality_analytics_vector-0.1.0/data_quality_analytics_vector.egg-info\n",
            "copying tests/__init__.py -> data_quality_analytics_vector-0.1.0/tests\n",
            "copying tests/test_etl.py -> data_quality_analytics_vector-0.1.0/tests\n",
            "copying data_quality_analytics_vector.egg-info/SOURCES.txt -> data_quality_analytics_vector-0.1.0/data_quality_analytics_vector.egg-info\n",
            "Writing data_quality_analytics_vector-0.1.0/setup.cfg\n",
            "Creating tar archive\n",
            "removing 'data_quality_analytics_vector-0.1.0' (and everything under it)\n",
            "\u001b[1m* Building wheel from sdist\u001b[0m\n",
            "\u001b[1m* Creating isolated environment: venv+pip...\u001b[0m\n",
            "\u001b[1m* Installing packages in isolated environment:\u001b[0m\n",
            "  - setuptools >= 40.8.0\n",
            "\u001b[1m* Getting build dependencies for wheel...\u001b[0m\n",
            "running egg_info\n",
            "writing data_quality_analytics_vector.egg-info/PKG-INFO\n",
            "writing dependency_links to data_quality_analytics_vector.egg-info/dependency_links.txt\n",
            "writing requirements to data_quality_analytics_vector.egg-info/requires.txt\n",
            "writing top-level names to data_quality_analytics_vector.egg-info/top_level.txt\n",
            "reading manifest file 'data_quality_analytics_vector.egg-info/SOURCES.txt'\n",
            "writing manifest file 'data_quality_analytics_vector.egg-info/SOURCES.txt'\n",
            "\u001b[1m* Building wheel...\u001b[0m\n",
            "running bdist_wheel\n",
            "running build\n",
            "running build_py\n",
            "creating build/lib/data_quality_analytics_vector\n",
            "copying data_quality_analytics_vector/etl.py -> build/lib/data_quality_analytics_vector\n",
            "copying data_quality_analytics_vector/quality_checks.py -> build/lib/data_quality_analytics_vector\n",
            "copying data_quality_analytics_vector/__init__.py -> build/lib/data_quality_analytics_vector\n",
            "creating build/lib/tests\n",
            "copying tests/test_etl.py -> build/lib/tests\n",
            "copying tests/__init__.py -> build/lib/tests\n",
            "installing to build/bdist.linux-x86_64/wheel\n",
            "running install\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/wheel\n",
            "creating build/bdist.linux-x86_64/wheel/data_quality_analytics_vector\n",
            "copying build/lib/data_quality_analytics_vector/etl.py -> build/bdist.linux-x86_64/wheel/./data_quality_analytics_vector\n",
            "copying build/lib/data_quality_analytics_vector/quality_checks.py -> build/bdist.linux-x86_64/wheel/./data_quality_analytics_vector\n",
            "copying build/lib/data_quality_analytics_vector/__init__.py -> build/bdist.linux-x86_64/wheel/./data_quality_analytics_vector\n",
            "creating build/bdist.linux-x86_64/wheel/tests\n",
            "copying build/lib/tests/test_etl.py -> build/bdist.linux-x86_64/wheel/./tests\n",
            "copying build/lib/tests/__init__.py -> build/bdist.linux-x86_64/wheel/./tests\n",
            "running install_egg_info\n",
            "running egg_info\n",
            "writing data_quality_analytics_vector.egg-info/PKG-INFO\n",
            "writing dependency_links to data_quality_analytics_vector.egg-info/dependency_links.txt\n",
            "writing requirements to data_quality_analytics_vector.egg-info/requires.txt\n",
            "writing top-level names to data_quality_analytics_vector.egg-info/top_level.txt\n",
            "reading manifest file 'data_quality_analytics_vector.egg-info/SOURCES.txt'\n",
            "writing manifest file 'data_quality_analytics_vector.egg-info/SOURCES.txt'\n",
            "Copying data_quality_analytics_vector.egg-info to build/bdist.linux-x86_64/wheel/./data_quality_analytics_vector-0.1.0-py3.10.egg-info\n",
            "running install_scripts\n",
            "creating build/bdist.linux-x86_64/wheel/data_quality_analytics_vector-0.1.0.dist-info/WHEEL\n",
            "creating '/content/data_quality_analytics_vector/dist/.tmp-hi4v3t0_/data_quality_analytics_vector-0.1.0-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "adding 'data_quality_analytics_vector/__init__.py'\n",
            "adding 'data_quality_analytics_vector/etl.py'\n",
            "adding 'data_quality_analytics_vector/quality_checks.py'\n",
            "adding 'tests/__init__.py'\n",
            "adding 'tests/test_etl.py'\n",
            "adding 'data_quality_analytics_vector-0.1.0.dist-info/METADATA'\n",
            "adding 'data_quality_analytics_vector-0.1.0.dist-info/WHEEL'\n",
            "adding 'data_quality_analytics_vector-0.1.0.dist-info/top_level.txt'\n",
            "adding 'data_quality_analytics_vector-0.1.0.dist-info/RECORD'\n",
            "removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[1m\u001b[92mSuccessfully built \u001b[4mdata_quality_analytics_vector-0.1.0.tar.gz\u001b[0m\u001b[1m\u001b[92m and \u001b[4mdata_quality_analytics_vector-0.1.0-py3-none-any.whl\u001b[0m\u001b[1m\u001b[92m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKKjlN2FVGLa",
        "outputId": "96f11458-9a1c-434c-921b-6a680fdc9470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing ./dist/data_quality_analytics_vector-0.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from data-quality-analytics-vector==0.1.0) (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->data-quality-analytics-vector==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->data-quality-analytics-vector==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->data-quality-analytics-vector==0.1.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->data-quality-analytics-vector==0.1.0) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->data-quality-analytics-vector==0.1.0) (1.16.0)\n",
            "Installing collected packages: data-quality-analytics-vector\n",
            "Successfully installed data-quality-analytics-vector-0.1.0\n"
          ]
        }
      ],
      "source": [
        "# Install and Test the Package Locally\n",
        "\n",
        "!pip install dist/data_quality_analytics_vector-0.1.0-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F84JlCXVGOA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from data_quality_analytics_vector.etl import load_data, transform_data, save_data\n",
        "from data_quality_analytics_vector.quality_checks import check_missing_values, check_duplicates\n",
        "\n",
        "# Sample usage\n",
        "df = pd.DataFrame({\"A\": [1, 2, None], \"B\": [4, None, 6], \"C\": [None, None, None]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfLtUXj296W7",
        "outputId": "8570ad66-4140-4f5d-ce92-2976ff14f6bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed DataFrame:\n",
            "      A    B     C\n",
            "0  1.0  4.0  None\n",
            "1  2.0  4.0  None\n",
            "2  2.0  6.0  None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/data_quality_analytics_vector/data_quality_analytics_vector/etl.py:12: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df.fillna(method=\"ffill\", inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# ETL Functions\n",
        "df = transform_data(df)\n",
        "print(\"Transformed DataFrame:\\n\", df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E5N9Wcc98VA",
        "outputId": "5165824a-324d-4e52-a80d-ebb76bc851c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values:\n",
            " A    0\n",
            "B    0\n",
            "C    3\n",
            "dtype: int64\n",
            "Duplicate rows: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# Quality Checks\n",
        "missing = check_missing_values(df)\n",
        "duplicates = check_duplicates(df)\n",
        "print(\"Missing values:\\n\", missing)\n",
        "print(\"Duplicate rows:\", duplicates)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
